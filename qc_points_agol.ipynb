{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e580909-bb3f-4f8c-b50a-3620d5318b61",
   "metadata": {},
   "source": [
    "## Quality Control Script for ArcGIS Pro Points Layer\n",
    "by Tara Wu, Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbb0ec-7656-4c25-ac89-3be274883dc2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Purpose:</b>  This script performs a series of quality checks for the Legacy Restoration Fund project in New England. As damage points along the Appalachian Trail are being collected, the resulting dataset will be checked for the following: \n",
    "<ul>\n",
    "    <li>nulls in specific fields invalid dates orphaned related records</li>\n",
    "    <li>missing related records </li>\n",
    "    <li> duplicates due to sync error,</li> \n",
    "    <li>inputs not in domains, repetitive attributes per user </li>\n",
    "    <li>offline data has been synced</li>\n",
    "    <li>proximity to trails/features</li> \n",
    "    <li>matching collector and region</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb592dcd-723d-4bd1-b40d-b7bdc331544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "# === import modules ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.gis import GIS\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# === log start time to measure elapsed time for full code ===\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# === configure variables ===\n",
    "# fields that shouldn't be null\n",
    "fc_required_fields = [\"GlobalID\", \"created_user\", \"created_date\", \"SHAPE\"]\n",
    "rt_required_fields = [\"defGlobalID\", \"Feature\", \"Feature_Action\"]\n",
    "\n",
    "# relevant dates (if used for future projects, change the start date)\n",
    "project_start_year = 2025  # YYYY\n",
    "project_start_month = 5  # M, no leading 0\n",
    "project_start_day = 5  # D, no leading 0\n",
    "\n",
    "# fields that suggest sync errors if containing duplicated data\n",
    "fc_sync_error_fields = [\"created_date\", \"created_user\"]\n",
    "rt_sync_error_fields = [\"CreationDate\", \"Creator\"]\n",
    "\n",
    "# domain dictionaries (np.nan = null in numpy/pandas)\n",
    "# codes from JSON as of 05/08\n",
    "# update this as appropriate when dropdowns are set in stone.\n",
    "fc_domain_dictionary = {\n",
    "    \"State\": [\"MA\", \"ME\", \"CT\", \"NH\", \"VT\"],  # not entered by user\n",
    "    \"Club\": [\n",
    "        \"AMC\",\n",
    "        \"AMC-CT\",\n",
    "        \"AMC-WMA\",\n",
    "        \"DOC\",\n",
    "        \"GMC\",\n",
    "        \"MATC\",\n",
    "        \"RMC\",\n",
    "    ],  # not entered by user\n",
    "    \"Evaluation_Code\": [\"Low\", \"Moderate\", \"High\", np.nan],\n",
    "    \"OnsiteMaterials\": [\"Yes\", \"No\", \"Maybe\", np.nan],  # field moved to related table\n",
    "    \"ConsiderRelocation\": [\"Yes\", \"No\", np.nan],\n",
    "}\n",
    "rt_domain_dictionary = {\n",
    "    \"Feature\": [\n",
    "        \"Bollard\",\n",
    "        \"Boulders\",\n",
    "        \"Cattle Guard\",\n",
    "        \"Checkdam – Rock\",\n",
    "        \"Checkdam – Wood\",\n",
    "        \"Checkdam – Concrete\",\n",
    "        \"Culvert – Rock\",\n",
    "        \"Culvert – Wood\",\n",
    "        \"Culvert – Concrete\",\n",
    "        \"Culvert – Metal\",\n",
    "        \"Culvert – Plastic\",\n",
    "        \"Drainage Dip / Swale\",\n",
    "        \"French Drain\",\n",
    "        \"Gate\",\n",
    "        \"Metal Rung\",\n",
    "        \"Ladder\",\n",
    "        \"Sign: Blaze Post\",\n",
    "        \"Sign: Bulletin Board / Kiosk\",\n",
    "        \"Sign: Directional / Wayfinding\",\n",
    "        \"Sign: Educational / Interpretative\",\n",
    "        \"Sign: Identification / Entrance\",\n",
    "        \"Sign: Regulatory / Safety\",\n",
    "        \"Step – Rock\",\n",
    "        \"Step – Wood\",\n",
    "        \"Step – Concrete\",\n",
    "        \"Step – Metal\",\n",
    "        \"Step Stones\",\n",
    "        \"Stile\",\n",
    "        \"Waterbar - Rock\",\n",
    "        \"Waterbar - Wood\",\n",
    "        \"Waterbar - Other\",\n",
    "        \"Boardwalk – Composite\",\n",
    "        \"Boardwalk – Metal (Grate)\",\n",
    "        \"Boardwalk – Wood\",\n",
    "        \"Corduroy – Wood\",\n",
    "        \"Native Stone Pavers\",\n",
    "        \"Non-Native Pavers\",\n",
    "        \"Puncheon / Bog Bridge – Wood\",\n",
    "        \"Retaining / Crib Wall – Rock\",\n",
    "        \"Retaining / Crib Wall – Wood\",\n",
    "        \"Retaining / Crib Wall – Metal\",\n",
    "        \"Turnpike / Causeway – Rock\",\n",
    "        \"Turnpike / Causeway – Wood\",\n",
    "        \"Riprap / Scree\",\n",
    "        \"Drainage Ditch\",\n",
    "        \"Fence – Barbwire / Slip Wire\",\n",
    "        \"Fence – Chain Link\",\n",
    "        \"Fence – Split Rail\",\n",
    "        \"Fence – Wire Mesh\",\n",
    "        \"Railing\",\n",
    "        \"Tread Benching\",\n",
    "        np.nan,\n",
    "    ],\n",
    "    \"FeatureAction\": [\"Build Add\", \"Repair Replace\", \"Remove\", np.nan],\n",
    "    \"Units\": [\"Each\", \"LinearFeet\", \"SquareFeet\", np.nan],\n",
    "    \"onsitematerials\": [\n",
    "        \"Yes\",\n",
    "        \"No\",\n",
    "        \"Maybe\",\n",
    "        np.nan,\n",
    "    ],  # previously a typo: Code = Maye, Description = Maybe.\n",
    "}\n",
    "\n",
    "# repetitive user input threshold\n",
    "threshold = 0.9\n",
    "\n",
    "# fields to look for repetitive inputs\n",
    "# update as necessary.  also see notes preceding repetitive input check\n",
    "fc_rep_error_fields = [\"Evaluation_Code\", \"Deficiency_Length\"]\n",
    "rt_rep_error_fields = [\"Feature\"]\n",
    "\n",
    "# buffer distance\n",
    "buffer_ft = 100\n",
    "\n",
    "# collector dictionary (maps usernames to associated trail club sections)\n",
    "collector_dict = {\n",
    "    \"twu_ATConservancy\": [\"MATC\"],\n",
    "    \"userB\": [\"RMC\", \"AMC\", \"DOC\"],\n",
    "    \"userC\": [\"GMC\"],\n",
    "    \"userD\": [\"AMC-WMA\"],\n",
    "    \"userE\": [\"AMC-CT\"],\n",
    "}\n",
    "\n",
    "# output filepath components\n",
    "current_date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "output_location = r\"D:\\...\\Error Reports\"  # UPDATE OUTPUT LOCATION HERE\n",
    "file_path = output_location + \"\\\\\" + f\"{current_date}_QC_summary.txt\"\n",
    "\n",
    "\n",
    "# output column order.  update if needs change\n",
    "fc_error_order = [\n",
    "    \"error_type\",\n",
    "    \"error_desc\",\n",
    "    \"OBJECTID\",\n",
    "    \"GlobalID\",\n",
    "    \"created_user\",\n",
    "    \"created_date\",\n",
    "    \"Deficiency_Length\",\n",
    "    \"Evaluation_Code\",\n",
    "    \"ConsiderRelocation\",\n",
    "    \"Notes\",\n",
    "    \"OnsiteMaterials\",\n",
    "    \"RelativeLinearLocation\",\n",
    "    \"SHAPE\",\n",
    "    \"MileMarker\",\n",
    "    \"State\",  # OnsiteMaterials is likely a remnant of old setup\n",
    "    \"LandOwner\",\n",
    "    \"OwnershipType\",\n",
    "    \"Club\",  # MileMarker and following are all nulls for now\n",
    "]\n",
    "# columns omitted from output to avoid confusion: \"last_edited_user\", \"last_edited_date\"\n",
    "rt_error_order = [\n",
    "    \"error_type\",\n",
    "    \"error_desc\",\n",
    "    \"OBJECTID\",\n",
    "    \"GlobalID\",\n",
    "    \"defGlobalID\",\n",
    "    \"CreationDate\",\n",
    "    \"Creator\",\n",
    "    \"Feature\",\n",
    "    \"Feature_Action\",\n",
    "    \"Quantity\",\n",
    "    \"Units\",\n",
    "    \"onsitematerials\",\n",
    "]\n",
    "# columns omitted from output to avoid confusion: \"EditDate\", \"Editor\"\n",
    "\n",
    "\n",
    "# === configure console and txt file output ===\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(file_path, mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# === connect to AGOL ===\n",
    "try:\n",
    "    gis = GIS(\n",
    "        \"pro\"\n",
    "    )  # runs on Pro via Jupyter notebooks or ArcGIS Pro's built-in Python\n",
    "    # gis = GIS(\"home\") # used in ArcGIS Notebooks (in AGOL) or ArcGIS Online Assistant\n",
    "except:\n",
    "    logging.info(\"X   Login error\\n\")\n",
    "else:\n",
    "    if gis.users.me is None:\n",
    "        logging.info(\"X   Not logged in to ArcGIS Pro. Please sign in.\\n\")\n",
    "    else:\n",
    "        logging.info(f\"    Login successful as {gis.users.me.username}\\n\")\n",
    "\n",
    "\n",
    "# === connect to feature service ===\n",
    "try:\n",
    "    # data from APPA_LRF_ProjectsV2\n",
    "    base_url = \"https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/APPA_LRF_ProjectsV2/FeatureServer\"\n",
    "\n",
    "    fc_url = f\"{base_url}/0\"  # LRF Tread Deficiency\n",
    "    rt_url = f\"{base_url}/1\"  # Related Table\n",
    "    fc_layer = FeatureLayer(fc_url)\n",
    "    rt_layer = FeatureLayer(rt_url)\n",
    "\n",
    "    # centerline data from APPA Features and Facilities\n",
    "    facilities_url = \"https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/ANST_Facilities/FeatureServer\"\n",
    "\n",
    "    tread_url = f\"{facilities_url}/7\"\n",
    "    tread_layer = FeatureLayer(tread_url)\n",
    "\n",
    "    # side trail data from APPA Features and Facilities\n",
    "    sidetrail_url = f\"{facilities_url}/6\"\n",
    "    sidetrail_layer = FeatureLayer(tread_url)\n",
    "\n",
    "    # other feature data from APPA Features and Facilities\n",
    "    feature_urls = [\n",
    "        f\"{facilities_url}/0\",  # bridges\n",
    "        f\"{facilities_url}/1\",  # campsites\n",
    "        f\"{facilities_url}/2\",  # parking\n",
    "        f\"{facilities_url}/3\",  # privies\n",
    "        f\"{facilities_url}/4\",  # shelters\n",
    "        f\"{facilities_url}/5\",\n",
    "    ]  # vistas\n",
    "except:\n",
    "    logging.info(\"X   Error loading URLs\\n\")\n",
    "else:\n",
    "    logging.info(\n",
    "        \"    URLs loaded for survey point data, related table, tread, side trail, and point features\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === load survey and line data into sdf ===\n",
    "try:\n",
    "    fc_features = fc_layer.query(where=\"1=1\", out_fields=\"*\", return_geometry=True).sdf\n",
    "    rt_features = rt_layer.query(where=\"1=1\", out_fields=\"*\").sdf\n",
    "\n",
    "    tread_features = tread_layer.query(\n",
    "        where=\"1=1\", out_fields=\"*\", return_geometry=True\n",
    "    ).sdf\n",
    "    sidetrail_features = sidetrail_layer.query(\n",
    "        where=\"1=1\", out_fields=\"*\", return_geometry=True\n",
    "    ).sdf\n",
    "except:\n",
    "    logging.info(\"X   Error converting to spatially-enabled data frames\\n\")\n",
    "else:\n",
    "    logging.info(\n",
    "        \"    Converted survey point data, related table, tread, and side trail to spatially-enabled data frames\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === QUALITY CHECKS ===\n",
    "\n",
    "fc_error_rows = []\n",
    "rt_error_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "\n",
    "# === null checks ===\n",
    "# function to check nulls, collect rows for error report\n",
    "def check_nulls(df, df_name, fields, error_list):\n",
    "    any_nulls = False\n",
    "    for field in fields:\n",
    "        if field in df.columns:\n",
    "            nulls = df[df[field].isnull()].copy()\n",
    "            if not nulls.empty:\n",
    "                any_nulls = True\n",
    "                nulls.loc[:, \"error_type\"] = \"nulls\"\n",
    "                nulls.loc[:, \"error_desc\"] = f\"NULL in {df_name} field: {field}\"\n",
    "                error_list.append(nulls)\n",
    "                message = f\"{df_name}.{field}: {len(nulls)} nulls\"\n",
    "                logging.info(\"        \" + message)\n",
    "                summary_rows.append(message)\n",
    "        else:\n",
    "            logging.info(f\"        {field} not in list of fields for {df_name}\")\n",
    "\n",
    "    if not any_nulls:\n",
    "        message = f\"0 null errors found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run null checks\n",
    "try:\n",
    "    summary_rows.append(\"Null errors:\")\n",
    "    check_nulls(fc_features, \"fc\", fc_required_fields, fc_error_rows)\n",
    "    check_nulls(rt_features, \"rt\", rt_required_fields, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running null check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Null check completed\\n\")\n",
    "\n",
    "\n",
    "# === date check ===\n",
    "# check for dates in the future or prior to the beginning of the project\n",
    "try:\n",
    "    project_start_date = datetime.datetime(\n",
    "        project_start_year, project_start_month, project_start_day\n",
    "    )\n",
    "    today = datetime.datetime.today()\n",
    "\n",
    "    fc_features[\"created_date\"] = pd.to_datetime(\n",
    "        fc_features[\"created_date\"], errors=\"coerce\"\n",
    "    )\n",
    "    invalid_dates = fc_features[\n",
    "        (fc_features[\"created_date\"] > today)\n",
    "        | (fc_features[\"created_date\"] < project_start_date)\n",
    "    ].copy()\n",
    "\n",
    "    if not invalid_dates.empty:\n",
    "        invalid_dates.loc[:, \"error_type\"] = \"dates\"\n",
    "        invalid_dates.loc[:, \"error_desc\"] = \"Invalid observation date\"\n",
    "        fc_error_rows.append(invalid_dates)\n",
    "    fc_error_rows.append(invalid_dates)\n",
    "    message = f\"{len(invalid_dates)} records with invalid dates\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Date errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running date check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "    logging.info(traceback.format_exc())\n",
    "else:\n",
    "    logging.info(\"    Date check completed\\n\")\n",
    "\n",
    "\n",
    "# === orphaned related records check ===\n",
    "# check for records in table that aren't related to a fc point\n",
    "try:\n",
    "    valid_ids = set(fc_features[\"GlobalID\"])\n",
    "    orphaned = rt_features[~rt_features[\"defGlobalID\"].isin(valid_ids)].copy()\n",
    "    orphaned.loc[:, \"error_type\"] = \"orphaned\"\n",
    "    orphaned.loc[:, \"error_desc\"] = \"Prescription record has no related tread feature\"\n",
    "    rt_error_rows.append(orphaned)\n",
    "    message = f\"{len(orphaned)} orphaned prescription records\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Orphaned record errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running orphaned records check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Orphaned related records check completed\\n\")\n",
    "\n",
    "\n",
    "# === missing related records check ===\n",
    "# check for fc points that aren't related to any records in the related table\n",
    "try:\n",
    "    related_counts = rt_features[\"defGlobalID\"].value_counts()\n",
    "    fc_features[\"related_count\"] = (\n",
    "        fc_features[\"GlobalID\"].map(related_counts).fillna(0).astype(int)\n",
    "    )\n",
    "    missing_related = fc_features[fc_features[\"related_count\"] == 0].copy()\n",
    "    missing_related.loc[:, \"error_type\"] = \"missing related\"\n",
    "    missing_related.loc[:, \"error_desc\"] = (\n",
    "        \"Tread feature has no related prescription records\"\n",
    "    )\n",
    "    fc_error_rows.append(missing_related)\n",
    "    message = f\"{len(missing_related)} features with no related prescriptions\"\n",
    "    logging.info(\"        \" + message)\n",
    "    summary_rows.append(\"Related record errors:\")\n",
    "    summary_rows.append(message)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running related records check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Missing related records check completed\\n\")\n",
    "\n",
    "\n",
    "# === sync error checks ===\n",
    "# function to check for sync errors via duplicated date/time and user fields\n",
    "# and collect rows for error report\n",
    "def check_sync_errors(df, df_name, fields, error_list):\n",
    "    any_sync_errors = False\n",
    "    dup_keys = df.groupby(fields).size().reset_index(name=\"count\")\n",
    "    dup_keys = dup_keys[dup_keys[\"count\"] > 1]\n",
    "\n",
    "    if not dup_keys.empty:\n",
    "        any_sync_errors = True\n",
    "        # Merge back to get full duplicate records\n",
    "        sync_errors = df.merge(dup_keys[fields], on=fields, how=\"inner\")\n",
    "        sync_errors.loc[:, \"error_type\"] = \"sync\"\n",
    "        sync_errors.loc[:, \"error_desc\"] = f\"Potential sync error in {df_name}\"\n",
    "        error_list.append(sync_errors)\n",
    "        message = f\"{len(sync_errors)} potential sync errors in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "    else:\n",
    "        logging.info(f\"        0 potential sync errors found in {df_name}\")\n",
    "\n",
    "    if not any_sync_errors:\n",
    "        summary_rows.append(f\"0 potential sync errors found in {df_name}\")\n",
    "\n",
    "\n",
    "# run sync error check\n",
    "try:\n",
    "    summary_rows.append(\"Potential sync errors:\")\n",
    "    check_sync_errors(fc_features, \"fc\", fc_sync_error_fields, fc_error_rows)\n",
    "    check_sync_errors(rt_features, \"rt\", rt_sync_error_fields, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running sync errors check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Sync error checks completed\\n\")\n",
    "\n",
    "\n",
    "# === domains check ===\n",
    "# function to check domains, collect rows for error report\n",
    "def check_domains(df, df_name, domain_dict, error_list):\n",
    "    any_domain_errors = False\n",
    "    for d in domain_dict:\n",
    "        if d in df.columns:\n",
    "            valid_values = domain_dict[d]\n",
    "            # Identify invalid rows (not in valid list and not null)\n",
    "            invalid_domain = df[~(df[d].isin(valid_values) | df[d].isnull())].copy()\n",
    "            if not invalid_domain.empty:\n",
    "                any_domain_errors = True\n",
    "                # Get unique invalid values for this column\n",
    "                unique_invalids = invalid_domain[d].dropna().unique()\n",
    "                invalid_domain.loc[:, \"error_type\"] = \"domains\"\n",
    "                invalid_domain.loc[:, \"error_desc\"] = invalid_domain[d].apply(\n",
    "                    lambda x: f\"'{x}' not in domains for {d}\"\n",
    "                )\n",
    "                error_list.append(invalid_domain)\n",
    "                message = f\"Invalid entries for field '{d}' in {df_name}: {list(unique_invalids)}\"\n",
    "                logging.info(\"        \" + message)\n",
    "                summary_rows.append(message)\n",
    "\n",
    "    if not any_domain_errors:\n",
    "        message = f\"0 domain errors found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run domain check\n",
    "try:\n",
    "    summary_rows.append(\"Domain errors:\")\n",
    "    check_domains(fc_features, \"fc\", fc_domain_dictionary, fc_error_rows)\n",
    "    check_domains(rt_features, \"rt\", rt_domain_dictionary, rt_error_rows)\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running domain check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Domain checks completed\\n\")\n",
    "\n",
    "\n",
    "# === repetitive attribute check ===\n",
    "# NOTE there is potential for this to flag false positives, e.g. if there just\n",
    "# happens to be a predominance of one feature type.  This will also\n",
    "# flag a false positive if a user only logs one point\n",
    "\n",
    "\n",
    "def check_repetitive_values(df, df_name, field_list, error_list, threshold):\n",
    "    # currently, there are different column names for fc and rt\n",
    "    if \"created_user\" in df.columns:\n",
    "        user_field = \"created_user\"\n",
    "    elif \"Creator\" in df.columns:\n",
    "        user_field = \"Creator\"\n",
    "    else:\n",
    "        logging.info(\"no username field found\")\n",
    "        return\n",
    "\n",
    "    # group by user\n",
    "    grouped = df.groupby(user_field)\n",
    "    rep_error_count = 0\n",
    "\n",
    "    # loop through each user's records\n",
    "    for user, group in grouped:\n",
    "        # check values in fields identified as having potential repetition\n",
    "        for field in field_list:\n",
    "            if field in group.columns:\n",
    "                # calculate frequency of value in field as proportion of user's total (how many times a particular user added x in a field / total number of records they logged)\n",
    "                value_counts = group[field].value_counts(normalize=True)\n",
    "\n",
    "                # filter for values with proportion above specified threshold\n",
    "                dominant_values = value_counts[value_counts > threshold]\n",
    "\n",
    "                # filter user's records for dominant values and append to error list\n",
    "                for val in dominant_values.index:\n",
    "                    rep_errors = group[group[field] == val].copy()\n",
    "                    rep_errors.loc[:, \"error_type\"] = \"repetitive\"\n",
    "                    rep_errors.loc[:, \"error_desc\"] = (\n",
    "                        f\"{user} repeated '{val}' in {df_name}.{field} over {int(threshold*100)}% of the time\"\n",
    "                    )\n",
    "                    error_list.append(rep_errors)\n",
    "                    rep_error_count += len(rep_errors)\n",
    "\n",
    "    if rep_error_count > 0:\n",
    "        message = f\"{rep_error_count} potentially repetitive inputs found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "    else:\n",
    "        message = f\"0 potentially repetitive inputs found in {df_name}\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "\n",
    "\n",
    "# run repetitive value check\n",
    "try:\n",
    "    summary_rows.append(\"Repetitive value errors:\")\n",
    "    check_repetitive_values(\n",
    "        fc_features, \"fc\", fc_rep_error_fields, fc_error_rows, threshold\n",
    "    )\n",
    "    check_repetitive_values(\n",
    "        rt_features, \"rt\", rt_rep_error_fields, rt_error_rows, threshold\n",
    "    )\n",
    "    summary_rows.append(\"\")  # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error running repetitive values check\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Repetitive values error checks completed\\n\")\n",
    "\n",
    "\n",
    "# === offline data sync check ===\n",
    "# NOTE this functionality is contingent on getting user logs from field staff.\n",
    "# then compare user log dates to AGOL dates\n",
    "\n",
    "\n",
    "# === proximity checks ===\n",
    "try:\n",
    "    # any_proximity_errors = False\n",
    "\n",
    "    # there seems to be an entry in fc_features with null SHAPE geometry.\n",
    "    # not sure how that happened, but this omits fc entries with null geometry\n",
    "    fc_features = fc_features[fc_features[\"SHAPE\"].notnull()].copy()\n",
    "\n",
    "    # Convert the SHAPE column to shapely geometry\n",
    "    fc_features[\"geometry\"] = fc_features[\"SHAPE\"].apply(shape)\n",
    "    tread_features[\"geometry\"] = tread_features[\"SHAPE\"].apply(shape)\n",
    "    sidetrail_features[\"geometry\"] = sidetrail_features[\"SHAPE\"].apply(shape)\n",
    "\n",
    "    # Convert points, AT treadway, side trails, and point features to GeoDataFrame\n",
    "    fc_gdf = gpd.GeoDataFrame(fc_features, geometry=\"geometry\", crs=3857)\n",
    "    tread_gdf = gpd.GeoDataFrame(tread_features, geometry=\"geometry\", crs=4269)\n",
    "    sidetrail_gdf = gpd.GeoDataFrame(sidetrail_features, geometry=\"geometry\", crs=4269)\n",
    "\n",
    "    # Reproject line features to match point CRS\n",
    "    tread_projected = tread_gdf.to_crs(epsg=3857)\n",
    "    sidetrail_projected = sidetrail_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # process point features to gdf with appropriate CRS\n",
    "    pointfeature_gdfs = []\n",
    "    for url in feature_urls:\n",
    "        fl = FeatureLayer(url)\n",
    "        sdf = fl.query(where=\"1=1\", out_fields=\"*\", return_geometry=True).sdf\n",
    "        sdf[\"geometry\"] = sdf[\"SHAPE\"].apply(shape)\n",
    "        gdf = gpd.GeoDataFrame(sdf, geometry=\"geometry\", crs=4269).to_crs(epsg=3857)\n",
    "        pointfeature_gdfs.append(gdf)\n",
    "\n",
    "    # Perform unary union on line features (tread and side trails)\n",
    "    all_lines_gdf = pd.concat([tread_projected, sidetrail_projected], ignore_index=True)\n",
    "\n",
    "    # Perform unary union on point features\n",
    "    all_points_gdf = pd.concat(pointfeature_gdfs, ignore_index=True)\n",
    "\n",
    "    # change buffer distance to m to match CRS\n",
    "    buffer_dist = buffer_ft * 0.3048\n",
    "\n",
    "    # Create buffer from all features (line and point)\n",
    "    trail_buffer = all_lines_gdf.geometry.buffer(\n",
    "        buffer_dist\n",
    "    )  # THIS LINE TAKES A FEW MINUTES WHEN RUN INDEPENDENTLY\n",
    "    point_buffer = all_points_gdf.geometry.buffer(buffer_dist)\n",
    "\n",
    "    # combine buffers (unary union on each, and then union between both)\n",
    "    combined_buffer = trail_buffer.unary_union.union(point_buffer.unary_union)\n",
    "\n",
    "    # Identify points outside the buffer\n",
    "    outside_points = fc_gdf[~fc_gdf.geometry.within(combined_buffer)].copy()\n",
    "\n",
    "    # Log errors\n",
    "    summary_rows.append(\"Proximity errors:\")\n",
    "    if not outside_points.empty:\n",
    "        any_proximity_errors = True\n",
    "        outside_points.loc[:, \"error_type\"] = \"proximity\"\n",
    "        outside_points.loc[:, \"error_desc\"] = \"Point located beyond buffer zone\"\n",
    "        fc_error_rows.append(outside_points)\n",
    "        message = f\"{len(outside_points)} points found beyond buffer zone\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")  # blank line between error checks\n",
    "    else:\n",
    "        message = f\"0 points found beyond buffer zone\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")  # blank line between error checks\n",
    "\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error finding survey points beyond buffer zone\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Proximity checks completed\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# === collector-region check ===\n",
    "# function to compare attributes\n",
    "def match_club(row):\n",
    "    allowed_clubs = collector_dict.get(row['created_user'], [])\n",
    "    return row['Acronym'] in allowed_clubs\n",
    "\n",
    "# run collector-region check\n",
    "try:\n",
    "    # Nearest spatial join: attach nearest Acronym from tread_projected to each point\n",
    "    fc_with_club = gpd.sjoin_nearest(\n",
    "        fc_gdf,\n",
    "        tread_projected[['Acronym', 'geometry']],\n",
    "        how='left',\n",
    "        distance_col='distance_to_trail'\n",
    "    )\n",
    "\n",
    "    club_mismatch = fc_with_club[~fc_with_club.apply(match_club, axis=1)].copy()\n",
    "\n",
    "    # log errors\n",
    "    summary_rows.append(\"Collector-region errors:\")\n",
    "\n",
    "    if not club_mismatch.empty:\n",
    "        club_mismatch[\"error_type\"] = \"collector-region\"\n",
    "        club_mismatch[\"error_desc\"] = \"Collector not assigned to this trail region\"\n",
    "        fc_error_rows.append(club_mismatch)\n",
    "        message = f\"{len(club_mismatch)} features with mismatched collector-region assignment\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")     # blank line between error checks\n",
    "    else:\n",
    "        message = f\"0 features with mismatched collector-region assignment\"\n",
    "        logging.info(\"        \" + message)\n",
    "        summary_rows.append(message)\n",
    "        summary_rows.append(\"\")     # blank line between error checks\n",
    "except Exception as e:\n",
    "    # Print the type of error and the error message\n",
    "    logging.info(\"X   Error finding collector-region mismatches\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "else:\n",
    "    logging.info(\"    Collector-region check completed\\n\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# === FINAL EXPORTS ===\n",
    "\n",
    "# === create master XLSX of all issues ===\n",
    "try:\n",
    "    source = [\"summary\", \"fc\", \"rt\"]\n",
    "    error_list = [summary_rows, fc_error_rows, rt_error_rows]\n",
    "    error_list_order = [None, fc_error_order, rt_error_order]\n",
    "\n",
    "    # GlobalID and defGlobalID\n",
    "    cols_to_cap = [\"GlobalID\", \"defGlobalID\"]\n",
    "\n",
    "    output_name = f\"{current_date}_all_errors.xlsx\"\n",
    "    output_path = output_location + \"\\\\\" + output_name\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "        for source_name, error_rows, order in zip(source, error_list, error_list_order):\n",
    "            if error_rows:\n",
    "                if source_name == \"summary\":\n",
    "                    all_errors = pd.DataFrame(error_rows, columns=[\"Summary\"])\n",
    "                else:\n",
    "                    all_errors = pd.concat(error_rows, ignore_index=True)\n",
    "                    # sort columns for output\n",
    "                    all_errors = all_errors[order]\n",
    "                    # capitalize all letters in GlobalID and defGlobalID columns\n",
    "                    for col in cols_to_cap:\n",
    "                        if col in all_errors.columns:\n",
    "                            all_errors[col] = all_errors[col].astype(str).str.upper()\n",
    "\n",
    "                all_errors.to_excel(writer, sheet_name=source_name, index=False)\n",
    "                if source_name == \"summary\":\n",
    "                    logging.info(\n",
    "                        f\"    Exported error summary to sheet '{source_name}' in {output_path}\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    logging.info(\n",
    "                        f\"    Exported {len(all_errors)} errors to sheet '{source_name}' in {output_path}\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                logging.info(\"    No errors found in {source_name}!\\n\")\n",
    "except Exception as e:\n",
    "    logging.info(\"X   Error creating XLSX of errors\\n\")\n",
    "    logging.info(f\"    Error type: {type(e).__name__}\")\n",
    "    logging.info(f\"    Error message: {e}\\n\")\n",
    "\n",
    "# Print confirmation messages\n",
    "logging.info(f\"    Exported output summary to {file_path}\\n\")\n",
    "print(f\"    Console outputs saved to {file_path}\")\n",
    "\n",
    "# calculate elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "hours = int(elapsed_time_sec // 3600)\n",
    "minutes = int((elapsed_time_sec % 3600) // 60)\n",
    "seconds = int(elapsed_time_sec % 60)\n",
    "\n",
    "\n",
    "logging.info(f\"    Elapsed Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
